# ğŸ§  Retrieval-Augmented Generation (RAG) System â€“ Local Setup

This project marks my first hands-on experience building a fully functional **Retrieval-Augmented Generation (RAG)** system from scratch. It deepened my understanding of how **LLMs, vector databases, and embedding models** work together to power document-based question answering â€” combining the strengths of both retrieval and generation approaches.

**ğŸ”§ What I Built**
**
ğŸ—‚ï¸ Phase 1: Knowledge Base in Open WebUI**

I started by uploading documents into **Open WebUI**, which allowed me to experiment with knowledge collections and see how LLMs can improve responses by using external context â€” without requiring retraining. This showed me the importance of retrieval-based augmentation for improving model accuracy.

**ğŸ Phase 2: Local RAG Pipeline with Python**

I then built a custom RAG pipeline using the following stack:
	â€¢	**Ollama** â€“ for running local LLMs
	â€¢	**LlamaIndex** â€“ to manage document queries and orchestration
	â€¢	**ChromaDB** â€“ as the vector database
	â€¢	**Hugging Face embeddings** â€“ to convert documents and queries into vector space

**ğŸ§ª Key Learnings & Challenges**

**âš™ï¸ Technical Insights**
	â€¢	Understood the difference between EphemeralClient vs. PersistentClient in **ChromaDB**.
	â€¢	My initial setup lost all vectors on restart.
	â€¢	Switching to PersistentClient enabled document reuse and made the system much faster.
	â€¢	Learned to fix dependency issues:
	â€¢	Faced a version error with the BAAI/bge-small-en model.
	â€¢	Solved it by aligning versions of transformers and tokenizers.
	â€¢	Explored different models:
	â€¢	Tested **llama3.2, deepseek-r1**, and others via Ollama.
	â€¢	Compared their response quality on domain-specific queries.
	â€¢	Custom Prompt Engineering:
	â€¢	Used PromptTemplate to tailor responses to my use case.
	â€¢	Tuned similarity_top_k to adjust how many relevant documents are retrieved.
	â€¢	Fixed missing dependencies (e.g., xlrd for reading Excel files).

**ğŸ” Concepts Reinforced**
	â€¢	How embedding models convert text into numerical vectors for semantic search.
	â€¢	How retrieval pipelines work:
	1.	Load and chunk documents
	2.	Create embeddings
	3.	Store vectors in ChromaDB
	4.	Query using a local LLM with contextual results


**ğŸ™Œ Outcome**

This project boosted my confidence in working with open-source AI tools and Python-based architectures. I now have a clear understanding of how to build, debug, and optimize a local RAG system â€” from document ingestion to response generation.
